{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eab558ab",
   "metadata": {},
   "source": [
    "# 2 Distributed Data Parallel Training\n",
    "\n",
    "## 2.1 Single-Node Distributed Communication in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5fac74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "def setup(rank, world_size):\n",
    "    os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
    "    os.environ[\"MASTER_PORT\"] = \"29500\"\n",
    "    dist.init_process_group(\"gloo\", rank=rank, world_size=world_size)\n",
    "\n",
    "def distributed_demo(rank, world_size):\n",
    "    setup(rank, world_size)\n",
    "    data = torch.randint(0, 10, (3,))\n",
    "    print(f\"rank {rank} data (before all-reduce): {data}\")\n",
    "    dist.all_reduce(data, async_op=False)\n",
    "    print(f\"rank {rank} data (after all-reduce): {data}\")\n",
    "\n",
    "world_size = 4\n",
    "mp.spawn(fn=distributed_demo, args=(world_size,), nprocs=world_size, join=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c72749",
   "metadata": {},
   "source": [
    "Node: a machine on the network\n",
    "\n",
    "Worker: an instance of a program that is participating in the distributed training. In this assignment, each worker will have a single process, so we will use worker, process, and worker process interchangeably. However, a worker may use multiple processes in practice.\n",
    "\n",
    "World size: the number of total workers in a process group.\n",
    "\n",
    "Global rank: an integer ID (between 0 and world_size-1) that uniquely identifies a worker in the process group. For example, for world size two, one process will have rank 0 (the master process) and the other process will have rank 1.\n",
    "\n",
    "Local world size: When running applications across different nodes, the local world size is the number of workers running locally on a given node. For example, if we have an application that spawns 4 workers on 2 nodes each, the world size would be 8 and the local world size would be 4. When running on a single node, the local world size is equivalent to the global world size.\n",
    "\n",
    "Local rank: An integer ID (between 0 and local_world_size-1) that uniquely identifies the index of a local worker on the machine.\n",
    "\n",
    "### 2.1.1 Best Practices for Benchmarking Distributed Applications\n",
    "\n",
    "Throughout this portion of the assignment you will be benchmarking distributed applications to better understand the overhead from communication. Here are a few best practices:\n",
    "- Whenever possible, run benchmarks on the same machine to facilitate controlled comparisons.\n",
    "- Perform several warm-up steps before timing the operation of interest. This is especially important for NCCL communication calls. 5 iterations of warmup is generally sufficient.\n",
    "- Call torch.cuda.synchronize() to wait for CUDA operations to complete when benchmarking on GPUs. Note that this is necessary even when calling communication operations with async_op=False, which returns when the operation is queued on the GPU (as opposed to when the communication actually finishes)\n",
    "- Timings may vary slightly across different ranks, so itâ€™s common to aggregate measurements across ranks to improve estimates. You may find the all-gather collective (specifically the dist.all_gather_object function) to be useful for collecting results from all ranks.\n",
    "- In general, debug locally with Gloo on CPU, and then as required in a given problem, benchmark with NCCL on GPU. Switching between the backends just involves changing the init_process_group call and tensor device casts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f563b40",
   "metadata": {},
   "source": [
    "**Benchmark Results (Macbook)**\n",
    "\n",
    "| Backend | Workers | Data Size | Time (s)      |\n",
    "|---------|---------|-----------|---------------|\n",
    "| gloo    | 2       | 1MB       | 0.002193      |\n",
    "| gloo    | 2       | 10MB      | 0.011302      |\n",
    "| gloo    | 2       | 100MB     | 0.091152      |\n",
    "| gloo    | 2       | 1000MB    | 0.893033      |\n",
    "| gloo    | 4       | 1MB       | 0.004053      |\n",
    "| gloo    | 4       | 10MB      | 0.022453      |\n",
    "| gloo    | 4       | 100MB     | 0.226364      |\n",
    "| gloo    | 4       | 1000MB    | 2.076443      |\n",
    "| gloo    | 6       | 1MB       | 0.007497      |\n",
    "| gloo    | 6       | 10MB      | 0.036168      |\n",
    "| gloo    | 6       | 100MB     | 0.382041      |\n",
    "| gloo    | 6       | 1000MB    | 3.864817      |\n",
    "| **NCCL** | -      | -         | Skipping NCCL: CUDA not available. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34ffac5",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
