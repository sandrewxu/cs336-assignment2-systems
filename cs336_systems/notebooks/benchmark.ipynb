{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19a1b8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2df487",
   "metadata": {},
   "source": [
    "# 1 Assignment Overview\n",
    "## 1.1 Profiling and Benchmarking\n",
    "### 1.1.3 End-to-End Benchmarking\n",
    "\n",
    "We start with forward and backward passes, 5 warmup steps, 10 benchmark steps. We notice low variance among measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a38dc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|        | forward_only   |   warmup_steps |   benchmark_steps |       avg |         std |\n",
      "|:-------|:---------------|---------------:|------------------:|----------:|------------:|\n",
      "| small  | False          |              5 |                10 | 0.038934  | 0.000192271 |\n",
      "| medium | False          |              5 |                10 | 0.0908162 | 0.000579289 |\n",
      "| large  | False          |              5 |                10 | 0.203657  | 0.000794752 |\n",
      "| xl     | False          |              5 |                10 | 0.393728  | 0.000980164 |\n",
      "| 2.7B   | False          |              5 |                10 | 0.556888  | 0.000197344 |\n"
     ]
    }
   ],
   "source": [
    "# context_length=256\n",
    "results_forwardandbackward_w5_n10 = {'small': {'forward_only': False, 'warmup_steps': 5, 'benchmark_steps': 10, 'avg': np.float64(0.038934001384768636), 'std': np.float64(0.00019227064981187754)}, 'medium': {'forward_only': False, 'warmup_steps': 5, 'benchmark_steps': 10, 'avg': np.float64(0.09081623799866065), 'std': np.float64(0.0005792887672763797)}, 'large': {'forward_only': False, 'warmup_steps': 5, 'benchmark_steps': 10, 'avg': np.float64(0.2036572418292053), 'std': np.float64(0.000794752277815363)}, 'xl': {'forward_only': False, 'warmup_steps': 5, 'benchmark_steps': 10, 'avg': np.float64(0.3937284264015034), 'std': np.float64(0.0009801636726452544)}, '2.7B': {'forward_only': False, 'warmup_steps': 5, 'benchmark_steps': 10, 'avg': np.float64(0.5568876736913808), 'std': np.float64(0.0001973441361972124)}}\n",
    "df = pd.DataFrame(results_forwardandbackward_w5_n10).T\n",
    "print(df.to_markdown())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2d59d4",
   "metadata": {},
   "source": [
    "Now, we look at only forward pass. We find that the times are around 1/3, which seems accurate since backward pass is around double the FLOPs of forward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f7b3a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|        | forward_only   |   warmup_steps |   benchmark_steps |       avg |         std |\n",
      "|:-------|:---------------|---------------:|------------------:|----------:|------------:|\n",
      "| small  | True           |              5 |                10 | 0.0157969 | 0.000125338 |\n",
      "| medium | True           |              5 |                10 | 0.0309509 | 0.000602889 |\n",
      "| large  | True           |              5 |                10 | 0.065458  | 7.11728e-05 |\n",
      "| xl     | True           |              5 |                10 | 0.127482  | 0.000120595 |\n",
      "| 2.7B   | True           |              5 |                10 | 0.174248  | 6.33902e-05 |\n"
     ]
    }
   ],
   "source": [
    "results_forwardonly_w5_n10 = {'small': {'forward_only': True, 'warmup_steps': 5, 'benchmark_steps': 10, 'avg': np.float64(0.01579690339276567), 'std': np.float64(0.00012533750080744773)}, 'medium': {'forward_only': True, 'warmup_steps': 5, 'benchmark_steps': 10, 'avg': np.float64(0.030950926861260088), 'std': np.float64(0.0006028892862904648)}, 'large': {'forward_only': True, 'warmup_steps': 5, 'benchmark_steps': 10, 'avg': np.float64(0.06545800276799127), 'std': np.float64(7.117279307500996e-05)}, 'xl': {'forward_only': True, 'warmup_steps': 5, 'benchmark_steps': 10, 'avg': np.float64(0.12748205178650096), 'std': np.float64(0.00012059490126578412)}, '2.7B': {'forward_only': True, 'warmup_steps': 5, 'benchmark_steps': 10, 'avg': np.float64(0.17424825453199447), 'std': np.float64(6.339024647792802e-05)}}\n",
    "df = pd.DataFrame(results_forwardonly_w5_n10).T\n",
    "print(df.to_markdown())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874cc584",
   "metadata": {},
   "source": [
    "Without warmup, we see a much higher standard deviation and also an impact on smaller / earlier models. This could be because there is overhead associated with the first couple of runs that is not an issue in the long run, which is what we care about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "406208e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|        | forward_only   |   warmup_steps |   benchmark_steps |       avg |        std |\n",
      "|:-------|:---------------|---------------:|------------------:|----------:|-----------:|\n",
      "| small  | False          |              0 |                10 | 0.0712939 | 0.0967123  |\n",
      "| medium | False          |              0 |                10 | 0.104675  | 0.0287744  |\n",
      "| large  | False          |              0 |                10 | 0.209273  | 0.0110618  |\n",
      "| xl     | False          |              0 |                10 | 0.397604  | 0.00777519 |\n",
      "| 2.7B   | False          |              0 |                10 | 0.562082  | 0.0147368  |\n"
     ]
    }
   ],
   "source": [
    "results_forwardandbackward_w0_n10 = {'small': {'forward_only': False, 'warmup_steps': 0, 'benchmark_steps': 10, 'avg': np.float64(0.0712938507203944), 'std': np.float64(0.09671228201616748)}, 'medium': {'forward_only': False, 'warmup_steps': 0, 'benchmark_steps': 10, 'avg': np.float64(0.10467507569119334), 'std': np.float64(0.0287743982342857)}, 'large': {'forward_only': False, 'warmup_steps': 0, 'benchmark_steps': 10, 'avg': np.float64(0.2092732895980589), 'std': np.float64(0.011061761055204388)}, 'xl': {'forward_only': False, 'warmup_steps': 0, 'benchmark_steps': 10, 'avg': np.float64(0.3976040959940292), 'std': np.float64(0.007775187865667317)}, '2.7B': {'forward_only': False, 'warmup_steps': 0, 'benchmark_steps': 10, 'avg': np.float64(0.5620815886883065), 'std': np.float64(0.014736830905364812)}}\n",
    "df = pd.DataFrame(results_forwardandbackward_w0_n10).T\n",
    "print(df.to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7e4a99c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|        | forward_only   |   warmup_steps |   benchmark_steps |       avg |        std |\n",
      "|:-------|:---------------|---------------:|------------------:|----------:|-----------:|\n",
      "| small  | True           |              0 |                10 | 0.0433155 | 0.0787202  |\n",
      "| medium | True           |              0 |                10 | 0.0442659 | 0.0324027  |\n",
      "| large  | True           |              0 |                10 | 0.0709446 | 0.011289   |\n",
      "| xl     | True           |              0 |                10 | 0.130531  | 0.00769949 |\n",
      "| 2.7B   | True           |              0 |                10 | 0.178989  | 0.0131833  |\n"
     ]
    }
   ],
   "source": [
    "results_forwardonly_w0_n10 = {'small': {'forward_only': True, 'warmup_steps': 0, 'benchmark_steps': 10, 'avg': np.float64(0.04331547362962738), 'std': np.float64(0.07872015609390083)}, 'medium': {'forward_only': True, 'warmup_steps': 0, 'benchmark_steps': 10, 'avg': np.float64(0.04426585491746664), 'std': np.float64(0.03240265923601385)}, 'large': {'forward_only': True, 'warmup_steps': 0, 'benchmark_steps': 10, 'avg': np.float64(0.07094462101813406), 'std': np.float64(0.011289025703075812)}, 'xl': {'forward_only': True, 'warmup_steps': 0, 'benchmark_steps': 10, 'avg': np.float64(0.13053130987100303), 'std': np.float64(0.007699488997647922)}, '2.7B': {'forward_only': True, 'warmup_steps': 0, 'benchmark_steps': 10, 'avg': np.float64(0.17898933509131892), 'std': np.float64(0.013183273384352073)}}\n",
    "df = pd.DataFrame(results_forwardonly_w0_n10).T\n",
    "print(df.to_markdown())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db94fb3",
   "metadata": {},
   "source": [
    "We see that even a couple of warmup steps helps standard deviation greatly and average greatly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80beab3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|        | forward_only   |   warmup_steps |   benchmark_steps |       avg |         std |\n",
      "|:-------|:---------------|---------------:|------------------:|----------:|------------:|\n",
      "| small  | False          |              2 |                10 | 0.0390191 | 0.000192001 |\n",
      "| medium | False          |              2 |                10 | 0.0909192 | 0.000975218 |\n",
      "| large  | False          |              2 |                10 | 0.204663  | 0.000475727 |\n",
      "| xl     | False          |              2 |                10 | 0.394708  | 0.000367019 |\n",
      "| 2.7B   | False          |              2 |                10 | 0.558024  | 0.000249775 |\n"
     ]
    }
   ],
   "source": [
    "results_forwardandbackward_w2_n10 = {'small': {'forward_only': False, 'warmup_steps': 2, 'benchmark_steps': 10, 'avg': np.float64(0.039019133895635605), 'std': np.float64(0.0001920012991025753)}, 'medium': {'forward_only': False, 'warmup_steps': 2, 'benchmark_steps': 10, 'avg': np.float64(0.0909191724145785), 'std': np.float64(0.0009752176591215706)}, 'large': {'forward_only': False, 'warmup_steps': 2, 'benchmark_steps': 10, 'avg': np.float64(0.20466322761494665), 'std': np.float64(0.0004757270254612791)}, 'xl': {'forward_only': False, 'warmup_steps': 2, 'benchmark_steps': 10, 'avg': np.float64(0.3947079855017364), 'std': np.float64(0.00036701886755863614)}, '2.7B': {'forward_only': False, 'warmup_steps': 2, 'benchmark_steps': 10, 'avg': np.float64(0.5580237713409588), 'std': np.float64(0.0002497751298431857)}}\n",
    "df = pd.DataFrame(results_forwardandbackward_w2_n10).T\n",
    "print(df.to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "760e1f9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|        | forward_only   |   warmup_steps |   benchmark_steps |       avg |         std |\n",
      "|:-------|:---------------|---------------:|------------------:|----------:|------------:|\n",
      "| small  | True           |              2 |                10 | 0.0159712 | 0.000284668 |\n",
      "| medium | True           |              2 |                10 | 0.030913  | 0.000638867 |\n",
      "| large  | True           |              2 |                10 | 0.0653365 | 0.000187829 |\n",
      "| xl     | True           |              2 |                10 | 0.127371  | 0.000170665 |\n",
      "| 2.7B   | True           |              2 |                10 | 0.174237  | 6.37191e-05 |\n"
     ]
    }
   ],
   "source": [
    "results_forwardonly_w2_n10 = {'small': {'forward_only': True, 'warmup_steps': 2, 'benchmark_steps': 10, 'avg': np.float64(0.01597115001641214), 'std': np.float64(0.00028466750118917957)}, 'medium': {'forward_only': True, 'warmup_steps': 2, 'benchmark_steps': 10, 'avg': np.float64(0.03091297169448808), 'std': np.float64(0.0006388669542270524)}, 'large': {'forward_only': True, 'warmup_steps': 2, 'benchmark_steps': 10, 'avg': np.float64(0.06533648789627478), 'std': np.float64(0.0001878294531717484)}, 'xl': {'forward_only': True, 'warmup_steps': 2, 'benchmark_steps': 10, 'avg': np.float64(0.1273713317932561), 'std': np.float64(0.000170664876891159)}, '2.7B': {'forward_only': True, 'warmup_steps': 2, 'benchmark_steps': 10, 'avg': np.float64(0.17423666048562153), 'std': np.float64(6.371911033421974e-05)}}\n",
    "df = pd.DataFrame(results_forwardonly_w2_n10).T\n",
    "print(df.to_markdown())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b172ceb2",
   "metadata": {},
   "source": [
    "### 1.1.4 Nsight Systems Profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3391094",
   "metadata": {},
   "source": [
    "**A.** Looking at the forward and backward passes, the times from Nsight are pretty much the same!\n",
    "\n",
    "We only ran out of memory on the 1024 context length XL and 2.7B model (without optimizer step).\n",
    "\n",
    "**B.** The `sm80_xmma_gemm_f32f32_f32f32_f32_tn_n_tilesize128x128x8_stage3_warpsize2x2x1_ffma_aligna4_alignc4_execute_kernel__5x_cublas` kernel takes the most time during the Large 1024 forward pass. It is called 145 times and takes up 46.8% of the time. Yes, this is the same kernel that takes up the most runtime in both forward and backward passes (17.0% though).\n",
    "\n",
    "**C.** There are a few element-wise kernels, but those max out at 4% of the time.\n",
    "\n",
    "**D.** Running with AdamW looks similar in terms of runtime to with backward pass. Everything is just a bit less. AdamW mostly uses element-wise kernels.\n",
    "\n",
    "**E.** Attention takes around 20% more time than softmax. However, softmax is much fewer FLOPs. This is weird.\n",
    "\n",
    "### 1.1.5 Mixed Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6e296ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.0001)\n",
      "tensor(9.9531, dtype=torch.float16)\n",
      "tensor(10.0021)\n",
      "tensor(10.0021)\n"
     ]
    }
   ],
   "source": [
    "s = torch.tensor(0,dtype=torch.float32)\n",
    "for i in range(1000):\n",
    "    s += torch.tensor(0.01,dtype=torch.float32)\n",
    "print(s)\n",
    "\n",
    "s = torch.tensor(0,dtype=torch.float16)\n",
    "for i in range(1000):\n",
    "    s += torch.tensor(0.01,dtype=torch.float16)\n",
    "print(s)\n",
    "\n",
    "s = torch.tensor(0,dtype=torch.float32)\n",
    "for i in range(1000):\n",
    "    s += torch.tensor(0.01,dtype=torch.float16)\n",
    "print(s)\n",
    "\n",
    "s = torch.tensor(0,dtype=torch.float32)\n",
    "for i in range(1000):\n",
    "    x = torch.tensor(0.01,dtype=torch.float16)\n",
    "    s += x.type(torch.float32)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882cd456",
   "metadata": {},
   "source": [
    "**=== Benchmarking Mixed Precision ===**\n",
    "\n",
    "**A.** Code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a63dca31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dtype of fc1.weight: torch.float32\n",
      "dtype of ln.weight: torch.float32\n",
      "dtype of fc2.weight: torch.float32\n",
      "dtype of first FF layer (fc1 output): torch.bfloat16\n",
      "dtype after layernorm: torch.float32\n",
      "dtype of logits (model output): torch.bfloat16\n",
      "dtype of loss: torch.float32\n",
      "dtype of gradient for fc1.weight: torch.float32\n",
      "dtype of gradient for ln.weight: torch.float32\n",
      "dtype of gradient for ln.bias: torch.float32\n",
      "dtype of gradient for fc2.weight: torch.float32\n",
      "Output shape: torch.Size([1, 20])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class ToyModel(nn.Module):\n",
    "    def __init__(self, in_features: int, out_features: int):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_features, 10, bias=False)\n",
    "        self.ln = nn.LayerNorm(10)\n",
    "        self.fc2 = nn.Linear(10, out_features, bias=False)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Print dtype of model parameters within the autocast context\n",
    "        print(f\"dtype of fc1.weight: {self.fc1.weight.dtype}\")\n",
    "        print(f\"dtype of ln.weight: {self.ln.weight.dtype}\")\n",
    "        print(f\"dtype of fc2.weight: {self.fc2.weight.dtype}\")\n",
    "        x = self.fc1(x)\n",
    "        print(f\"dtype of first FF layer (fc1 output): {x.dtype}\")\n",
    "        x = self.relu(x)\n",
    "        x = self.ln(x)\n",
    "        print(f\"dtype after layernorm: {x.dtype}\")\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Model parameters are initialized in FP32\n",
    "model = ToyModel(5, 20).to('cuda')\n",
    "ins = torch.arange(5, dtype=torch.float32, device='cuda').unsqueeze(0)  # Add batch dimension for LN\n",
    "\n",
    "# We'll compute a dummy loss as well\n",
    "target = torch.zeros((1, 20), dtype=torch.float32, device='cuda')\n",
    "\n",
    "with torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16):\n",
    "    out = model(ins)\n",
    "    print(f\"dtype of logits (model output): {out.dtype}\")\n",
    "    # Use MSELoss as dummy loss function, will also print dtype\n",
    "    loss_fn = nn.MSELoss()\n",
    "    loss = loss_fn(out, target)\n",
    "    print(f\"dtype of loss: {loss.dtype}\")\n",
    "\n",
    "# Backward to get gradients\n",
    "loss.backward()\n",
    "# Print gradients dtypes for all parameters\n",
    "for name, param in model.named_parameters():\n",
    "    # param.grad may be None if unused\n",
    "    grad_dtype = param.grad.dtype if param.grad is not None else None\n",
    "    print(f\"dtype of gradient for {name}: {grad_dtype}\")\n",
    "\n",
    "print(f\"Output shape: {out.shape}\")\n",
    "\n",
    "# Summary comment (for reference, not printed in code):\n",
    "# - Model parameters: FP32 throughout\n",
    "# - fc1 output: FP16 (autocast active)\n",
    "# - layernorm output: FP32 (LayerNorm always returns FP32)\n",
    "# - Model logits (fc2 output): FP16 (autocast active)\n",
    "# - Loss: FP32 (reduction with FP32 accumulation)\n",
    "# - Model gradients: FP32 (computed in FP32 for stability)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f39d57",
   "metadata": {},
   "source": [
    "Only the matmuls are downcasted.\n",
    "\n",
    "**B.** LayerNorm is special because of the variance (sum of squares). This can overflow the max value of float16. BF16 keeps the exponent of float32 which helps with this problem. However, then, the decimal point precision may be too small. That's another issue here.\n",
    "\n",
    "**C.** Mixed precision is amazing on the 2.7B 256, but actually seems to be a bit laggier than regular on small and medium. Perhaps this is because the matmuls take so little time that it's not worth the conversion time.\n",
    "\n",
    "### 1.1.6 Profiling Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc58dcd1",
   "metadata": {},
   "source": [
    "**A.** There are clear separations between forward pass, backward pass, and optimizer. \n",
    "\n",
    "**B.** Peak memory usages of forward pass (2.7B): 128: 46GB; 256: 53GB; 512: 70GB; in full step, always goes to 55GB by optimizer step.\n",
    "\n",
    "**C.** Using mixed precision, 512 goes to 67GB. Optimizer memory does not change. This is not huge but significant.\n",
    "\n",
    "**D.** Size of Transformer residual: BxCxD = 4 x 512 x 2560 = 5242880 params * 4 bytes / 1024^2 = 20MB\n",
    "\n",
    "**E.** Most are coming from autograd, it seems. Also attention! Including softmax."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7daccb",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
